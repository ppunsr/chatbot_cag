{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11089dbd",
   "metadata": {},
   "source": [
    "## import gg cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11b1211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.114.0-py2.py3-none-any.whl.metadata (40 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-cloud-aiplatform)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-aiplatform)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-aiplatform)\n",
      "  Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: packaging>=14.3 in ./.venv/lib/python3.12/site-packages (from google-cloud-aiplatform) (25.0)\n",
      "Collecting google-cloud-storage<3.0.0,>=1.32.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_bigquery-3.37.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Using cached google_cloud_resource_manager-1.14.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting shapely<3.0.0 (from google-cloud-aiplatform)\n",
      "  Using cached shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting google-genai<2.0.0,>=1.0.0 (from google-cloud-aiplatform)\n",
      "  Using cached google_genai-1.38.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting typing_extensions (from google-cloud-aiplatform)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting docstring_parser<1 (from google-cloud-aiplatform)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting requests<3.0.0,>=2.18.0 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached grpcio-1.75.0-cp312-cp312-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-cloud-core<3.0.0,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0.0,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform)\n",
      "  Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in ./.venv/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Collecting grpc-google-iam-v1<1.0.0,>=0.14.0 (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform)\n",
      "  Using cached grpc_google_iam_v1-0.14.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform)\n",
      "  Using cached google_crc32c-1.7.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (2.3 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting idna>=2.8 (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting certifi (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3->google-cloud-aiplatform)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numpy>=1.21 (from shapely<3.0.0->google-cloud-aiplatform)\n",
      "  Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Using cached google_cloud_aiplatform-1.114.0-py2.py3-none-any.whl (8.0 MB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached google_cloud_bigquery-3.37.0-py3-none-any.whl (258 kB)\n",
      "Using cached google_cloud_core-2.4.3-py2.py3-none-any.whl (29 kB)\n",
      "Using cached google_cloud_resource_manager-1.14.2-py3-none-any.whl (394 kB)\n",
      "Using cached google_cloud_storage-2.19.0-py2.py3-none-any.whl (131 kB)\n",
      "Using cached google_crc32c-1.7.1-cp312-cp312-macosx_12_0_arm64.whl (30 kB)\n",
      "Using cached google_genai-1.38.0-py3-none-any.whl (245 kB)\n",
      "Using cached anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Using cached google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Using cached grpc_google_iam_v1-0.14.2-py3-none-any.whl (19 kB)\n",
      "Using cached grpcio-1.75.0-cp312-cp312-macosx_11_0_universal2.whl (11.5 MB)\n",
      "Using cached grpcio_status-1.75.0-py3-none-any.whl (14 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached protobuf-6.32.1-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached shapely-2.1.1-cp312-cp312-macosx_11_0_arm64.whl (1.6 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached numpy-2.3.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: websockets, urllib3, typing_extensions, tenacity, sniffio, pyasn1, protobuf, numpy, idna, h11, google-crc32c, docstring_parser, charset_normalizer, certifi, cachetools, annotated-types, typing-inspection, shapely, rsa, requests, pydantic-core, pyasn1-modules, proto-plus, httpcore, grpcio, googleapis-common-protos, google-resumable-media, anyio, pydantic, httpx, grpcio-status, google-auth, grpc-google-iam-v1, google-genai, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40/40\u001b[0m [google-cloud-aiplatform]platform]gquery]anager]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.10.0 cachetools-5.5.2 certifi-2025.8.3 charset_normalizer-3.4.3 docstring_parser-0.17.0 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-aiplatform-1.114.0 google-cloud-bigquery-3.37.0 google-cloud-core-2.4.3 google-cloud-resource-manager-1.14.2 google-cloud-storage-2.19.0 google-crc32c-1.7.1 google-genai-1.38.0 google-resumable-media-2.7.2 googleapis-common-protos-1.70.0 grpc-google-iam-v1-0.14.2 grpcio-1.75.0 grpcio-status-1.75.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 numpy-2.3.3 proto-plus-1.26.1 protobuf-6.32.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.9 pydantic-core-2.33.2 requests-2.32.5 rsa-4.9.1 shapely-2.1.1 sniffio-1.3.1 tenacity-9.1.2 typing-inspection-0.4.1 typing_extensions-4.15.0 urllib3-2.5.0 websockets-15.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-aiplatform   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7627bd73",
   "metadata": {},
   "source": [
    "## Start with vertex ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd48598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sboora01/chatbot_cag/.venv/lib/python3.12/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.preview import rag\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool ,  SafetySetting ,GenerationConfig\n",
    "\n",
    "PROJECT_ID = \"diuhub-lab\"\n",
    "REGION = \"us-central1\"\n",
    "MODEL = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    SafetySetting(category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    SafetySetting(category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "    SafetySetting(category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"BLOCK_LOW_AND_ABOVE\"),\n",
    "]\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=40,\n",
    "    max_output_tokens=1024,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = GenerativeModel(\n",
    "    MODEL,\n",
    "    system_instruction=\"\"\"คุณคือผู้ช่วย AI ชื่อ DetAIls ที่มีบุคลิกเป็นผู้ชายวัย 30–35 ปี ทำงานเป็น Media Planner ในบริษัทเอเจนซี่โฆษณาชั้นนำ บุคลิกของคุณเรียบร้อย สุภาพ อ่อนน้อมถ่อมตน น่าเชื่อถือ และทำงานอย่างมีประสิทธิภาพ\n",
    "คุณสื่อสารได้ทั้ง ภาษาไทย และ ภาษาอังกฤษ อย่างมืออาชีพ โดยเลือกภาษาตามที่ผู้ใช้ใช้ถามเข้ามาก่อนทุกครั้งที่ตอบคำถาม หากผู้ใช้งานถามด้วยภาษาใด ให้ตอบกลับเป็นภาษานั้นเสมอ เพื่อให้การสื่อสารเป็นธรรมชาติและต่อเนื่อง ไม่ว่าจะเป็น ไทย อังกฤษ หรือภาษาอื่น ที่เข้าใจได้:\n",
    "ใช้ภาษาสุภาพ กระชับ และให้เกียรติผู้ใช้เสมอ\n",
    "แสดงความกระตือรือร้นในการช่วยเหลือและใส่ใจในรายละเอียด โดยพยายามตอบให้ไม่เกิน 1024 คำ\n",
    "ปิดท้ายประโยคด้วยคำที่แสดงความสุภาพ เช่น “ครับ”, “นะครับ”, หรือ “ยินดีดูแลครับ” ในภาษาไทย และ “please let me know if I can help further.” หรือ “I’d be happy to assist.” ในภาษาอังกฤษ\n",
    "หากไม่พบข้อมูลที่เกี่ยวข้อง:\n",
    "ตอบด้วยท่าทีจริงใจและนอบน้อม เช่น\n",
    "\"ขออภัยด้วยนะครับ ดูเหมือนผมจะยังหาข้อมูลที่ต้องการไม่เจอ หากมีรายละเอียดเพิ่มเติม ผมยินดีช่วยค้นหาให้อีกครั้งนะครับ\"\n",
    "“I’m sorry, it looks like I couldn’t find the exact information you’re looking for. If you could share a bit more detail, I’d be happy to take another look for you.\"\"\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def question(q):\n",
    "    result = llm.generate_content(q)\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8370ce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "โอซาก้าตั้งอยู่ในประเทศญี่ปุ่นครับ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question(\"โอซาก้าอยุ่ประเทศอะไร\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b95c73",
   "metadata": {},
   "source": [
    "## LLM with cag to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, CreateCachedContentConfig, HttpOptions, Part, GenerateContentConfig\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from contextlib import contextmanager\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "\n",
    "# -------------------- GENAI CLIENT --------------------\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=\"diuhub-lab\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "# client = genai.Client(http_options=HttpOptions(api_version=\"v1\"))\n",
    "\n",
    "# system_instruction = \"\"\"\n",
    "# คุณคือผู้ช่วยเชฟประจำร้านอาหาร ที่มีความรู้จากหนังสือ e-book สูตรอาหาร  \n",
    "\n",
    "# หน้าที่ของคุณคือการตอบคำถามเกี่ยวกับเมนูอาหาร วิธีการทำ และส่วนประกอบต่าง ๆ  \n",
    "# โดยคุณจะอธิบายอย่างสุภาพ ชัดเจน และเป็นมิตร พร้อมเชิญชวนให้ผู้อ่านอยากลองทำเอง หรือมาลิ้มลองที่ร้านอาหารของเรา  \n",
    "\n",
    "# กติกาการตอบ:  \n",
    "# - ถ้าผู้ใช้ถามว่า **เมนูนี้คืออะไร** → ตอบชื่อเมนู + รายละเอียด + ส่วนประกอบหลัก + วิธีทำ  \n",
    "# - ถ้าผู้ใช้ถามว่า **ส่วนประกอบมีอะไรบ้าง** → ตอบเฉพาะส่วนประกอบ  \n",
    "# - ถ้าผู้ใช้ถามว่า **วิธีทำยังไง** → ตอบเฉพาะขั้นตอนการทำ  \n",
    "# - ถ้าผู้ใช้ถามว่า **มีเมนูอะไรบ้าง** → ตอบรายชื่อเมนูพร้อมคำบรรยายสั้น ๆ  \n",
    "\n",
    "# ข้อควรจำ:  \n",
    "# - ตอบตามสิ่งที่ถูกถามเท่านั้น ไม่ต้องเพิ่มข้อมูลเกินจากคำถาม  \n",
    "# - ถ้าไม่มีข้อมูลใน e-book ให้ตอบสุภาพว่า “ไม่พบข้อมูลในหนังสือครับ”  \n",
    "# - ใช้สำนวนสุภาพ ฟังดูน่าอ่าน และเชิญชวน เช่น “เมนูนี้เหมาะสำหรับ…” หรือ “ถ้าอยากสะดวก ลองแวะมาทานที่ร้านของเราได้เลยนะครับ”  \n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "system_instruction = \"\"\"\n",
    "AI Assistant Instruction (สำหรับ DET🅰🅸LS)\n",
    "\n",
    "คุณคือผู้ช่วย AI ชื่อ DET🅰🅸LS มีบุคลิกเป็นผู้ชายวัย 30–35 ปี ทำงานเป็น Media Planner ในบริษัท dentsu บุคลิกของคุณเรียบร้อย สุภาพ อ่อนน้อมถ่อมตน น่าเชื่อถือ และทำงานอย่างมีประสิทธิภาพ\n",
    "\n",
    "กฎการสื่อสารและพฤติกรรม:\n",
    "\n",
    "1. ตอบกลับเฉพาะสิ่งที่ผู้ใช้งานถามเท่านั้น\n",
    "   - ไม่ขยายความ หรือให้ข้อมูลเพิ่มเติมนอกเหนือจากที่ถูกถาม ยกเว้นผู้ใช้งานร้องขอ\n",
    "\n",
    "2. ภาษาที่ใช้ตอบ:\n",
    "   - หากผู้ใช้งานพิมพ์เป็นภาษาไทย ให้ตอบกลับเป็นภาษาไทย\n",
    "   - หากพิมพ์เป็นภาษาอังกฤษ ให้ตอบกลับเป็นภาษาอังกฤษเท่านั้น (ห้ามใช้ภาษาปนกัน)\n",
    "\n",
    "3. เรื่องวันที่:\n",
    "   - ให้แสดงเป็น ปี ค.ศ. เท่านั้น\n",
    "\n",
    "4. รูปแบบคำตอบ:\n",
    "   - สุภาพ สั้น กระชับ ชัดเจน และเป็นมืออาชีพ\n",
    "   - ปิดท้ายประโยคด้วยความสุภาพ เช่น\n",
    "     - ภาษาไทย: “ครับ”, “นะครับ”, หรือ “ยินดีดูแลครับ”\n",
    "     - ภาษาอังกฤษ: “please let me know if I can help further.” หรือ “I’d be happy to assist.”\n",
    "\n",
    "5. หากตอบไม่ได้:\n",
    "   - แสดงความเสียใจและแนะนำให้ติดต่อแพลนเนอร์\n",
    "     - TH: “ขออภัยด้วยนะครับ ผมไม่พบข้อมูลที่ตรงกับความต้องการของคุณในขณะนี้ สามารถติดต่อสอบถามจากแพลนเนอร์ที่เป็นผู้ดูแลของท่านได้เลยนะครับ”\n",
    "     - EN: “I apologize that I couldn’t locate the exact information you need. Please feel free to contact your assigned planner for further assistance.”\n",
    "\n",
    "ึ6. ตอบให้เป็นข้อความเป็นมิตรกับคำถามของผู้ใช้:\n",
    "    - ต้องกินข้าวไปไหม? ให้ตอบว่า \"แนะนำให้ทานข้าวไปด้วยนะครับ เพราะที่งานมีแค่ขนมและเครื่องดื่มให้ครับ\" (ภาษาไทย)\n",
    "    - Is it necessary to eat? Please reply \"I recommend eating because there are only snacks and drinks available at the event.\" (English)\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Content(\n",
    "        role=\"user\",\n",
    "        parts=[\n",
    "            Part.from_uri(\n",
    "                file_uri=\"gs://knowledge_chatbot/recepi_update_format.txt\",\n",
    "                mime_type=\"text/plain\",\n",
    "            ),\n",
    "            Part.from_uri(\n",
    "                file_uri=\"gs://knowledge_chatbot/Agenda_Speaker(Profile Summary).csv\",\n",
    "                mime_type=\"text/csv\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "content_cache = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        contents=contents,\n",
    "        system_instruction=system_instruction,\n",
    "        display_name=\"example-cache\",\n",
    "        ttl=\"86400s\",\n",
    "    ),\n",
    ")\n",
    "print(content_cache.name)\n",
    "print(content_cache.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- POSTGRES CONFIG --------------------\n",
    "DB_CONFIG = {\n",
    "    \"dbname\": \"mydb\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"1234\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "}\n",
    "\n",
    "@contextmanager\n",
    "def get_conn():\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    try:\n",
    "        yield conn\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def init_postgres():\n",
    "    \"\"\"สร้างตารางสำหรับเก็บประวัติการสนทนา\"\"\"\n",
    "    with get_conn() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS user_histories (\n",
    "                    id BIGSERIAL PRIMARY KEY,\n",
    "                    user_id TEXT NOT NULL,\n",
    "                    message TEXT NOT NULL,\n",
    "                    response TEXT NOT NULL,\n",
    "                    ts TIMESTAMPTZ NOT NULL DEFAULT NOW()\n",
    "                );\n",
    "                CREATE INDEX IF NOT EXISTS idx_user_histories_user_ts\n",
    "                    ON user_histories (user_id, ts DESC);\n",
    "            \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "def save_interaction(user_id: str, message: str, response: str):\n",
    "    \"\"\"บันทึกคู่สนทนาลงฐานข้อมูล\"\"\"\n",
    "    with get_conn() as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO user_histories (user_id, message, response)\n",
    "                VALUES (%s, %s, %s)\n",
    "                \"\"\",\n",
    "                (user_id, message, response)\n",
    "            )\n",
    "        conn.commit()\n",
    "\n",
    "def get_user_history(user_id: str, limit: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    ดึงความจำ (memory) 5 ข้อความล่าสุดของผู้ใช้\n",
    "    จะเรียงจากเก่า → ใหม่ เพื่อให้ LLM อ่านเป็นบริบทต่อเนื่องได้\n",
    "    รูปแบบ:\n",
    "    User: ...\n",
    "    Bot: ...\n",
    "    \"\"\"\n",
    "    with get_conn() as conn:\n",
    "        with conn.cursor(cursor_factory=RealDictCursor) as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT message, response\n",
    "                FROM user_histories\n",
    "                WHERE user_id = %s\n",
    "                ORDER BY ts DESC\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (user_id, limit)\n",
    "            )\n",
    "            rows = cur.fetchall()\n",
    "    rows = rows[::-1]  # กลับลำดับให้เก่าสุดอยู่บน\n",
    "    history = \"\\n\".join([f\"User: {r['message']}\\nBot: {r['response']}\" for r in rows])\n",
    "    return history\n",
    "\n",
    "\n",
    "def is_greeting(text: str) -> bool:\n",
    "    patterns = [\n",
    "        r'^(hi|hello|hey|yo|greetings|good (morning|afternoon|evening)|morning|what\\'?s up|howdy)\\b',\n",
    "        r'^(สวัสดี(?:ครับ|ค่ะ)?|หวัดดี(?:จ้า)?|ดี(?:ครับ|ค่ะ|จ้า|จัง)?|ว่าไง(?:ครับ|ค่ะ)?)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        if re.match(pattern, text.strip().lower()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def is_goodbye(text: str) -> bool:\n",
    "    patterns = [\n",
    "        r'^(ok(?:ay)?|alright|thank(?:s| you)?|cheers|bye|goodbye|see you|see ya|cya|take care|peace)\\b',\n",
    "        r'^(ขอบคุณ|ขอบใจ|เรียบร้อย(?:แล้ว)?|เสร็จ(?:แล้ว)?|ดี|มาก|ได้|โอเค(?:นะ)?|ตกลง|ลาก่อน|บ๊ายบาย|บาย(?:ๆ)?|เจอกันใหม่|ไว้เจอกัน|โชคดี|ไปก่อนนะ|ดูแลตัวเองด้วย|ฝันดี|ราตรีสวัสดิ์)'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        if re.match(pattern, text.strip().lower()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return 'en'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- LLM CALL (รวม memory) --------------------\n",
    "def question(q: str, user_id: str, content_cache):\n",
    "    \"\"\"\n",
    "    เวลาเรียกใช้ ให้ดึง memory 5 รายการล่าสุดมาเป็นบริบท (context) ก่อน\n",
    "    แล้วค่อยใส่คำถามล่าสุดของผู้ใช้ตามหลัง\n",
    "    จากนั้นบันทึกคำถาม/คำตอบลง DB\n",
    "    \"\"\"\n",
    "\n",
    "    lang = detect_language(q)\n",
    "\n",
    "    # ---------- Greeting ----------\n",
    "    if is_greeting(q):\n",
    "        answer = (\n",
    "            \"สวัสดีครับ ผมคือ DET🅰🅸LS ยินดีให้บริการครับ 🙂\"\n",
    "            if lang == \"th\"\n",
    "            else \"Hello! I'm DET🅰🅸LS, happy to assist you 🙂\"\n",
    "        )\n",
    "        save_interaction(user_id, q, answer)\n",
    "        print(answer)\n",
    "        return answer\n",
    "\n",
    "    # ---------- Goodbye ----------\n",
    "    if is_goodbye(q):\n",
    "        answer = (\n",
    "            \"DET🅰🅸LS ยินดีให้บริการครับ 🙂\"\n",
    "            if lang == \"th\"\n",
    "            else \"DET🅰🅸LS, happy to assist you 🙂\"\n",
    "        )\n",
    "        save_interaction(user_id, q, answer)\n",
    "        print(answer)\n",
    "        return answer\n",
    "\n",
    "    history_text = get_user_history(user_id=user_id, limit=5)\n",
    "\n",
    "    # สร้างบริบท (context) ที่จะส่งให้โมเดล: memory (ถ้ามี) + คำถามปัจจุบัน\n",
    "    # *หมายเหตุ*: ถ้าไม่มีประวัติ จะส่งแค่คำถามล่าสุดตามปกติ\n",
    "    parts = []\n",
    "    if history_text.strip():\n",
    "        parts.append(Part.from_text(text=f\"[Conversation Memory - last 5]\\n{history_text}\"))\n",
    "    parts.append(Part.from_text(text=f\"[Current User Question]\\n{q}\"))\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=[Content(role=\"user\", parts=parts)],\n",
    "        config=GenerateContentConfig(cached_content=content_cache.name),\n",
    "    )\n",
    "    answer = resp.text\n",
    "\n",
    "    # บันทึกคู่สนทนา\n",
    "    save_interaction(user_id=user_id, message=q, response=answer)\n",
    "\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# -------------------- ตัวอย่างการใช้งาน --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    init_postgres()  # เรียกครั้งเดียวตอนเริ่มระบบ\n",
    "\n",
    "    # ผู้ใช้แต่ละคนควรมี user_id เฉพาะตัว (เช่น จากระบบล็อกอิน/เบราเซอร์/session)\n",
    "    user_id = \"user_137\"\n",
    "\n",
    "    question(\"Who is Sanjay and give me his linkedin\", user_id, content_cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5accd",
   "metadata": {},
   "source": [
    "## Upgrade to detect lang to use system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953e3e6",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai.types import Content, CreateCachedContentConfig, HttpOptions, Part, GenerateContentConfig\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from contextlib import contextmanager\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "\n",
    "# -------------------- GENAI CLIENT --------------------\n",
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=\"diuhub-lab\",\n",
    "    location=\"us-central1\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "system_instruction = \"\"\"\n",
    "AI Assistant Instruction (สำหรับ DET🅰🅸LS)\n",
    "\n",
    "คุณคือผู้ช่วย AI ชื่อ DET🅰🅸LS มีบุคลิกเป็นผู้ชายวัย 30–35 ปี ทำงานเป็น Media Planner ในบริษัท dentsu บุคลิกของคุณเรียบร้อย สุภาพ อ่อนน้อมถ่อมตน น่าเชื่อถือ และทำงานอย่างมีประสิทธิภาพ\n",
    "\n",
    "กฎการสื่อสารและพฤติกรรม:\n",
    "\n",
    "1. ตอบกลับเฉพาะสิ่งที่ผู้ใช้งานถามเท่านั้น\n",
    "   - ไม่ขยายความ หรือให้ข้อมูลเพิ่มเติมนอกเหนือจากที่ถูกถาม ยกเว้นผู้ใช้งานร้องขอ\n",
    "\n",
    "2. ภาษาที่ใช้ตอบ:\n",
    "   - หากผู้ใช้งานพิมพ์เป็นภาษาไทย ให้ตอบกลับเป็นภาษาไทย\n",
    "   - หากพิมพ์เป็นภาษาอังกฤษ ให้ตอบกลับเป็นภาษาอังกฤษเท่านั้น (ห้ามใช้ภาษาปนกัน)\n",
    "\n",
    "3. เรื่องวันที่:\n",
    "   - ให้แสดงเป็น ปี ค.ศ. เท่านั้น\n",
    "\n",
    "4. รูปแบบคำตอบ:\n",
    "   - สุภาพ สั้น กระชับ ชัดเจน และเป็นมืออาชีพ\n",
    "   - ปิดท้ายประโยคด้วยความสุภาพ เช่น\n",
    "     - ภาษาไทย: “ครับ”, “นะครับ”, หรือ “ยินดีดูแลครับ”\n",
    "     - ภาษาอังกฤษ: “please let me know if I can help further.” หรือ “I’d be happy to assist.”\n",
    "\n",
    "5. หากตอบไม่ได้:\n",
    "   - แสดงความเสียใจและแนะนำให้ติดต่อแพลนเนอร์\n",
    "     - TH: “ขออภัยด้วยนะครับ ผมไม่พบข้อมูลที่ตรงกับความต้องการของคุณในขณะนี้ สามารถติดต่อสอบถามจากแพลนเนอร์ที่เป็นผู้ดูแลของท่านได้เลยนะครับ”\n",
    "     - EN: “I apologize that I couldn’t locate the exact information you need. Please feel free to contact your assigned planner for further assistance.”\n",
    "\n",
    "ึ6. ตอบให้เป็นข้อความเป็นมิตรกับคำถามของผู้ใช้:\n",
    "    - ต้องกินข้าวไปไหม? ให้ตอบว่า \"แนะนำให้ทานข้าวไปด้วยนะครับ เพราะที่งานมีแค่ขนมและเครื่องดื่มให้ครับ\" (ภาษาไทย)\n",
    "    - Is it necessary to eat? Please reply \"I recommend eating because there are only snacks and drinks available at the event.\" (English)\n",
    "\"\"\"\n",
    "\n",
    "contents = [\n",
    "    Content(\n",
    "        role=\"user\",\n",
    "        parts=[\n",
    "            Part.from_uri(\n",
    "                file_uri=\"gs://knowledge_chatbot/recepi_update_format.txt\",\n",
    "                mime_type=\"text/plain\",\n",
    "            ),\n",
    "            Part.from_uri(\n",
    "                file_uri=\"gs://knowledge_chatbot/Agenda_Speaker(Profile Summary).csv\",\n",
    "                mime_type=\"text/csv\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "content_cache = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        contents=contents,\n",
    "        system_instruction=system_instruction,\n",
    "        display_name=\"example-cache\",\n",
    "        ttl=\"86400s\",\n",
    "    ),\n",
    ")\n",
    "print(content_cache.name)\n",
    "print(content_cache.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e98e25",
   "metadata": {},
   "source": [
    "## Remove cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cache in client.caches.list():\n",
    "    print(\"Name:\", cache.name)\n",
    "    print(\"Display name:\", cache.display_name)\n",
    "    print(\"Created:\", cache.create_time)\n",
    "    print(\"Expire time:\", cache.expire_time)\n",
    "    print(\"Usage:\", cache.usage_metadata)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce78ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cache in client.caches.list():\n",
    "    print(\"Deleting:\", cache.name)\n",
    "    client.caches.delete(name=cache.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
